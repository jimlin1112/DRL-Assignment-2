{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over, final score: 14664\n",
      "Game over, final score: 12236\n",
      "Game over, final score: 7172\n",
      "Game over, final score: 14668\n",
      "Game over, final score: 7104\n",
      "Game over, final score: 5736\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit\n",
    "\n",
    "COLOR_MAP = {\n",
    "    0: \"#cdc1b4\", 2: \"#eee4da\", 4: \"#ede0c8\", 8: \"#f2b179\",\n",
    "    16: \"#f59563\", 32: \"#f67c5f\", 64: \"#f65e3b\", 128: \"#edcf72\",\n",
    "    256: \"#edcc61\", 512: \"#edc850\", 1024: \"#edc53f\", 2048: \"#edc22e\",\n",
    "    4096: \"#3c3a32\", 8192: \"#3c3a32\", 16384: \"#3c3a32\", 32768: \"#3c3a32\"\n",
    "}\n",
    "TEXT_COLOR = {\n",
    "    2: \"#776e65\", 4: \"#776e65\", 8: \"#f9f6f2\", 16: \"#f9f6f2\",\n",
    "    32: \"#f9f6f2\", 64: \"#f9f6f2\", 128: \"#f9f6f2\", 256: \"#f9f6f2\",\n",
    "    512: \"#f9f6f2\", 1024: \"#f9f6f2\", 2048: \"#f9f6f2\", 4096: \"#f9f6f2\"\n",
    "}\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compress_and_merge(row, score):\n",
    "    size = 4\n",
    "    # 過濾非零元素\n",
    "    temp = np.zeros(size, dtype=np.int32)\n",
    "    pos = 0\n",
    "    for i in range(size):\n",
    "        if row[i] != 0:\n",
    "            temp[pos] = row[i]\n",
    "            pos += 1\n",
    "\n",
    "    # 合併相鄰相同元素\n",
    "    result = np.zeros(size, dtype=np.int32)\n",
    "    write_pos = 0\n",
    "    i = 0\n",
    "    while i < pos:\n",
    "        if i + 1 < pos and temp[i] == temp[i + 1]:\n",
    "            result[write_pos] = temp[i] * 2\n",
    "            score += temp[i] * 2\n",
    "            i += 2\n",
    "        else:\n",
    "            result[write_pos] = temp[i]\n",
    "            i += 1\n",
    "        write_pos += 1\n",
    "\n",
    "    return result, score\n",
    "\n",
    "@jit(nopython=True)\n",
    "def move_board(board, direction, score):\n",
    "    new_board = board.copy()\n",
    "    moved = False\n",
    "    if direction == 0:  # 上\n",
    "        for j in range(4):\n",
    "            col, new_score = compress_and_merge(new_board[:, j], score)\n",
    "            if not np.array_equal(col, new_board[:, j]):\n",
    "                moved = True\n",
    "            new_board[:, j] = col\n",
    "            score = new_score\n",
    "    elif direction == 1:  # 下\n",
    "        for j in range(4):\n",
    "            col = new_board[::-1, j]\n",
    "            col, new_score = compress_and_merge(col, score)\n",
    "            if not np.array_equal(col, new_board[::-1, j]):\n",
    "                moved = True\n",
    "            new_board[::-1, j] = col\n",
    "            score = new_score\n",
    "    elif direction == 2:  # 左\n",
    "        for i in range(4):\n",
    "            row, new_score = compress_and_merge(new_board[i], score)\n",
    "            if not np.array_equal(row, new_board[i]):\n",
    "                moved = True\n",
    "            new_board[i] = row\n",
    "            score = new_score\n",
    "    elif direction == 3:  # 右\n",
    "        for i in range(4):\n",
    "            row = new_board[i, ::-1]\n",
    "            row, new_score = compress_and_merge(row, score)\n",
    "            if not np.array_equal(row, new_board[i, ::-1]):\n",
    "                moved = True\n",
    "            new_board[i, ::-1] = row\n",
    "            score = new_score\n",
    "    return new_board, moved, score\n",
    "\n",
    "class Game2048Env(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Game2048Env, self).__init__()\n",
    "\n",
    "        self.size = 4\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.score = 0\n",
    "\n",
    "        # Action space: 0: up, 1: down, 2: left, 3: right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "        self.last_move_valid = True\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.score = 0\n",
    "        self.add_random_tile()\n",
    "        self.add_random_tile()\n",
    "        return self.board\n",
    "\n",
    "    def add_random_tile(self):\n",
    "        # empty_cells = list(zip(*np.where(self.board == 0)))\n",
    "        # if empty_cells:\n",
    "        #     x, y = random.choice(empty_cells)\n",
    "        #     self.board[x, y] = 2 if random.random() < 0.9 else 4\n",
    "        empty_cells = np.where(self.board == 0)\n",
    "        if len(empty_cells[0]) > 0:\n",
    "            idx = random.randint(0, len(empty_cells[0]) - 1)\n",
    "            x, y = empty_cells[0][idx], empty_cells[1][idx]\n",
    "            self.board[x, y] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "    def compress(self, row):\n",
    "        new_row = row[row != 0]\n",
    "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
    "        return new_row\n",
    "\n",
    "    def merge(self, row):\n",
    "        for i in range(len(row) - 1):\n",
    "            if row[i] == row[i + 1] and row[i] != 0:\n",
    "                row[i] *= 2\n",
    "                row[i + 1] = 0\n",
    "                self.score += row[i]\n",
    "        return row\n",
    "\n",
    "    def move_left(self):\n",
    "        moved = False\n",
    "        for i in range(self.size):\n",
    "            original_row = self.board[i].copy()\n",
    "            new_row = self.compress(self.board[i])\n",
    "            new_row = self.merge(new_row)\n",
    "            new_row = self.compress(new_row)\n",
    "            self.board[i] = new_row\n",
    "            if not np.array_equal(original_row, self.board[i]):\n",
    "                moved = True\n",
    "        return moved\n",
    "\n",
    "    def move_right(self):\n",
    "        moved = False\n",
    "        for i in range(self.size):\n",
    "            original_row = self.board[i].copy()\n",
    "            reversed_row = self.board[i][::-1]\n",
    "            reversed_row = self.compress(reversed_row)\n",
    "            reversed_row = self.merge(reversed_row)\n",
    "            reversed_row = self.compress(reversed_row)\n",
    "            self.board[i] = reversed_row[::-1]\n",
    "            if not np.array_equal(original_row, self.board[i]):\n",
    "                moved = True\n",
    "        return moved\n",
    "\n",
    "    def move_up(self):\n",
    "        moved = False\n",
    "        for j in range(self.size):\n",
    "            original_col = self.board[:, j].copy()\n",
    "            col = self.compress(self.board[:, j])\n",
    "            col = self.merge(col)\n",
    "            col = self.compress(col)\n",
    "            self.board[:, j] = col\n",
    "            if not np.array_equal(original_col, self.board[:, j]):\n",
    "                moved = True\n",
    "        return moved\n",
    "\n",
    "    def move_down(self):\n",
    "        moved = False\n",
    "        for j in range(self.size):\n",
    "            original_col = self.board[:, j].copy()\n",
    "            reversed_col = self.board[:, j][::-1]\n",
    "            reversed_col = self.compress(reversed_col)\n",
    "            reversed_col = self.merge(reversed_col)\n",
    "            reversed_col = self.compress(reversed_col)\n",
    "            self.board[:, j] = reversed_col[::-1]\n",
    "            if not np.array_equal(original_col, self.board[:, j]):\n",
    "                moved = True\n",
    "        return moved\n",
    "\n",
    "    def is_game_over(self):\n",
    "        if np.any(self.board == 0):\n",
    "            return False\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size - 1):\n",
    "                if self.board[i, j] == self.board[i, j+1]:\n",
    "                    return False\n",
    "        for j in range(self.size):\n",
    "            for i in range(self.size - 1):\n",
    "                if self.board[i, j] == self.board[i+1, j]:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"Invalid action\"\n",
    "\n",
    "        # if action == 0:\n",
    "        #     moved = self.move_up()\n",
    "        # elif action == 1:\n",
    "        #     moved = self.move_down()\n",
    "        # elif action == 2:\n",
    "        #     moved = self.move_left()\n",
    "        # elif action == 3:\n",
    "        #     moved = self.move_right()\n",
    "        # else:\n",
    "        #     moved = False\n",
    "\n",
    "        self.board, moved, self.score = move_board(self.board, action, self.score)\n",
    "\n",
    "        self.last_move_valid = moved\n",
    "\n",
    "        if moved:\n",
    "            self.add_random_tile()\n",
    "\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        return self.board, self.score, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\", action=None):\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim(-0.5, self.size - 0.5)\n",
    "        ax.set_ylim(-0.5, self.size - 0.5)\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                value = self.board[i, j]\n",
    "                color = COLOR_MAP.get(value, \"#3c3a32\")\n",
    "                text_color = TEXT_COLOR.get(value, \"white\")\n",
    "                rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, facecolor=color, edgecolor=\"black\")\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                if value != 0:\n",
    "                    ax.text(j, i, str(value), ha='center', va='center',\n",
    "                            fontsize=16, fontweight='bold', color=text_color)\n",
    "        title = f\"score: {self.score}\"\n",
    "        if action is not None:\n",
    "            title += f\" | action: {self.actions[action]}\"\n",
    "        plt.title(title)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    def simulate_row_move(self, row):\n",
    "        new_row = row[row != 0]\n",
    "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
    "        for i in range(len(new_row) - 1):\n",
    "            if new_row[i] == new_row[i + 1] and new_row[i] != 0:\n",
    "                new_row[i] *= 2\n",
    "                new_row[i + 1] = 0\n",
    "        new_row = new_row[new_row != 0]\n",
    "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
    "        return new_row\n",
    "\n",
    "\n",
    "    def is_move_legal(self, action):\n",
    "        temp_board = self.board.copy()\n",
    "\n",
    "        # if action == 0:  # Move up\n",
    "        #     for j in range(self.size):\n",
    "        #         col = temp_board[:, j]\n",
    "        #         new_col = self.simulate_row_move(col)\n",
    "        #         temp_board[:, j] = new_col\n",
    "        # elif action == 1:  # Move down\n",
    "        #     for j in range(self.size):\n",
    "        #         col = temp_board[:, j][::-1]\n",
    "        #         new_col = self.simulate_row_move(col)\n",
    "        #         temp_board[:, j] = new_col[::-1]\n",
    "        # elif action == 2:  # Move left\n",
    "        #     for i in range(self.size):\n",
    "        #         row = temp_board[i]\n",
    "        #         temp_board[i] = self.simulate_row_move(row)\n",
    "        # elif action == 3:  # Move right\n",
    "        #     for i in range(self.size):\n",
    "        #         row = temp_board[i][::-1]\n",
    "        #         new_row = self.simulate_row_move(row)\n",
    "        #         temp_board[i] = new_row[::-1]\n",
    "        # else:\n",
    "        #     raise ValueError(\"Invalid action\")\n",
    "        # return not np.array_equal(self.board, temp_board)\n",
    "\n",
    "        new_board, moved, _ = move_board(temp_board, action, self.score)\n",
    "        return moved\n",
    "\n",
    "def rot90(pattern, board_size):\n",
    "    return [(j, board_size - 1 - i) for (i, j) in pattern]\n",
    "\n",
    "def rot180(pattern, board_size):\n",
    "    return [(board_size - 1 - i, board_size - 1 - j) for (i, j) in pattern]\n",
    "\n",
    "def rot270(pattern, board_size):\n",
    "    return [(board_size - 1 - j, i) for (i, j) in pattern]\n",
    "\n",
    "def reflect(pattern, board_size):\n",
    "    return [(i, board_size - 1 - j) for (i, j) in pattern]\n",
    "\n",
    "class NTupleApproximator:\n",
    "    def __init__(self, board_size, patterns):\n",
    "        \"\"\"\n",
    "        Initializes the N-Tuple approximator.\n",
    "        Hint: you can adjust these if you want\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.patterns = patterns\n",
    "        # Create a weight dictionary for each pattern (shared within a pattern group)\n",
    "        self.weights = [defaultdict(float) for _ in patterns]\n",
    "        # Generate symmetrical transformations for each pattern\n",
    "        self.symmetry_patterns = []\n",
    "        self.symmetry_groups = []\n",
    "        for pattern in self.patterns:\n",
    "            syms = self.generate_symmetries(pattern)\n",
    "            self.symmetry_groups.append(syms)\n",
    "            for syms_ in syms:\n",
    "                self.symmetry_patterns.append(syms_)\n",
    "\n",
    "    def generate_symmetries(self, pattern):\n",
    "        # TODO: Generate 8 symmetrical transformations of the given pattern.\n",
    "        board_size = self.board_size\n",
    "        sym0 = pattern\n",
    "        sym1 = rot90(pattern, board_size)\n",
    "        sym2 = rot180(pattern, board_size)\n",
    "        sym3 = rot270(pattern, board_size)\n",
    "        syms = [sym0, sym1, sym2, sym3,\n",
    "              reflect(sym0, board_size),\n",
    "              reflect(sym1, board_size),\n",
    "              reflect(sym2, board_size),\n",
    "              reflect(sym3, board_size)]\n",
    "        return syms\n",
    "\n",
    "\n",
    "    def tile_to_index(self, tile):\n",
    "        \"\"\"\n",
    "        Converts tile values to an index for the lookup table.\n",
    "        \"\"\"\n",
    "        if tile == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(math.log(tile, 2))\n",
    "\n",
    "    def get_feature(self, board, coords):\n",
    "        # TODO: Extract tile values from the board based on the given coordinates and convert them into a feature tuple.\n",
    "        return tuple(self.tile_to_index(board[i, j]) for (i, j) in coords)\n",
    "\n",
    "\n",
    "    def value(self, board):\n",
    "        # TODO: Estimate the board value: sum the evaluations from all patterns.\n",
    "        total_value = 0.0\n",
    "        for i, syms in enumerate(self.symmetry_groups):\n",
    "            group_value = 0.0\n",
    "            for pattern in syms:\n",
    "                feature = self.get_feature(board, pattern)\n",
    "                group_value += self.weights[i][feature]\n",
    "            total_value += group_value / len(syms)\n",
    "        return total_value\n",
    "\n",
    "    def update(self, board, delta, alpha):\n",
    "        # TODO: Update weights based on the TD error.\n",
    "        for i, syms in enumerate(self.symmetry_groups):\n",
    "            update_value = alpha * delta / len(syms)\n",
    "            for pattern in syms:\n",
    "                feature = self.get_feature(board, pattern)\n",
    "                self.weights[i][feature] += update_value\n",
    "\n",
    "# Note: This MCTS implementation is almost identical to the previous one,\n",
    "# except for the rollout phase, which now incorporates the approximator.\n",
    "\n",
    "# Node for TD-MCTS using the TD-trained value approximator\n",
    "class TD_MCTS_Node:\n",
    "    def __init__(self, state, score, parent=None, action=None):\n",
    "        \"\"\"\n",
    "        state: current board state (numpy array)\n",
    "        score: cumulative score at this node\n",
    "        parent: parent node (None for root)\n",
    "        action: action taken from parent to reach this node\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.score = score\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0.0\n",
    "        # List of untried actions based on the current state's legal moves\n",
    "        self.untried_actions = [a for a in range(4) if env.is_move_legal(a)]\n",
    "\n",
    "    def fully_expanded(self):\n",
    "        # A node is fully expanded if no legal actions remain untried.\n",
    "        return len(self.untried_actions) == 0\n",
    "\n",
    "\n",
    "# TD-MCTS class utilizing a trained approximator for leaf evaluation\n",
    "class TD_MCTS:\n",
    "    def __init__(self, env, approximator, iterations=500, exploration_constant=1.41, rollout_depth=10, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.approximator = approximator\n",
    "        self.iterations = iterations\n",
    "        self.c = exploration_constant\n",
    "        self.rollout_depth = rollout_depth\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def create_env_from_state(self, state, score):\n",
    "        # Create a deep copy of the environment with the given state and score.\n",
    "        new_env = copy.deepcopy(self.env)\n",
    "        new_env.board = state.copy()\n",
    "        new_env.score = score\n",
    "        return new_env\n",
    "\n",
    "    def select_child(self, node):\n",
    "        # TODO: Use the UCT formula: Q + c * sqrt(log(parent.visits)/child.visits) to select the best child.\n",
    "        best_value = -float('inf')\n",
    "        best_child = None\n",
    "        for action, child in node.children.items():\n",
    "          if child.visits == 0:\n",
    "              uct_value = float('inf')\n",
    "          else:\n",
    "              uct_value = (child.total_reward / child.visits) + self.c * math.sqrt(math.log(node.visits) / child.visits)\n",
    "          if uct_value > best_value:\n",
    "              best_value = uct_value\n",
    "              best_child = child\n",
    "        return best_child\n",
    "\n",
    "\n",
    "    def rollout(self, sim_env, depth):\n",
    "        # TODO: Perform a random rollout until reaching the maximum depth or a terminal state.\n",
    "        # TODO: Use the approximator to evaluate the final state.\n",
    "        current_depth = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        discount = 1.0\n",
    "\n",
    "        while current_depth < self.rollout_depth and not done:\n",
    "            legal_moves = [a for a in range(4) if sim_env.is_move_legal(a)]\n",
    "            if not legal_moves:\n",
    "                break\n",
    "\n",
    "            action = random.choice(legal_moves)\n",
    "            _, reward, done, _ = sim_env.step(action)\n",
    "            total_reward += discount * reward\n",
    "            discount *= self.gamma\n",
    "            current_depth += 1\n",
    "\n",
    "        # Use TD approximator to evaluate final state\n",
    "        final_value = self.approximator.value(sim_env.board)\n",
    "        discounted_final = (self.gamma ** current_depth) * final_value\n",
    "        return total_reward + discounted_final\n",
    "\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        # TODO: Propagate the obtained reward back up the tree.\n",
    "        current = node\n",
    "        while current is not None:\n",
    "            current.visits += 1\n",
    "            current.total_reward += reward\n",
    "            reward *= self.gamma\n",
    "            current = current.parent\n",
    "\n",
    "\n",
    "    def run_simulation(self, root):\n",
    "        node = root\n",
    "        sim_env = self.create_env_from_state(node.state, node.score)\n",
    "\n",
    "        # TODO: Selection: Traverse the tree until reaching an unexpanded node.\n",
    "        while node.fully_expanded() and node.children:\n",
    "            node = self.select_child(node)\n",
    "            sim_env.step(node.action)\n",
    "\n",
    "\n",
    "        # TODO: Expansion: If the node is not terminal, expand an untried action.\n",
    "        legal_actions = [a for a in range(4) if sim_env.is_move_legal(a)]\n",
    "        if legal_actions and node.untried_actions:\n",
    "            action = random.choice(node.untried_actions)\n",
    "            node.untried_actions.remove(action)\n",
    "            new_state, reward, done, _ = sim_env.step(action)\n",
    "            new_score = sim_env.score\n",
    "            child_node = TD_MCTS_Node(new_state, new_score, parent=node, action=action)\n",
    "            node.children[action] = child_node\n",
    "            node = child_node\n",
    "\n",
    "\n",
    "        # Rollout: Simulate a random game from the expanded node.\n",
    "        rollout_reward = self.rollout(sim_env, self.rollout_depth)\n",
    "        # Backpropagate the obtained reward.\n",
    "        self.backpropagate(node, rollout_reward)\n",
    "\n",
    "    def best_action_distribution(self, root):\n",
    "        # Compute the normalized visit count distribution for each child of the root.\n",
    "        total_visits = sum(child.visits for child in root.children.values())\n",
    "        distribution = np.zeros(4)\n",
    "        best_visits = -1\n",
    "        best_action = None\n",
    "        for action, child in root.children.items():\n",
    "            distribution[action] = child.visits / total_visits if total_visits > 0 else 0\n",
    "            if child.visits > best_visits:\n",
    "                best_visits = child.visits\n",
    "                best_action = action\n",
    "        return best_action, distribution\n",
    "\n",
    "env = Game2048Env()\n",
    "with open(\"ntuple_approximator.pkl\", \"rb\") as f:\n",
    "    approximator = pickle.load(f)\n",
    "def test_play_2048():\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    td_mcts = TD_MCTS(env, approximator, iterations=50, exploration_constant=1.41, rollout_depth=10, gamma=0.99)\n",
    "    while not done:\n",
    "        root = TD_MCTS_Node(state, env.score)\n",
    "        for _ in range(td_mcts.iterations):\n",
    "            td_mcts.run_simulation(root)\n",
    "        best_act, _ = td_mcts.best_action_distribution(root)\n",
    "        state, reward, done, _ = env.step(best_act)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # while not done:\n",
    "    #     legal_moves = [a for a in range(4) if env.is_move_legal(a)]\n",
    "    #     if not legal_moves:\n",
    "    #         break\n",
    "    #     afterstate_values_mean = []\n",
    "    #     for a in legal_moves:\n",
    "    #         afterstates = []\n",
    "    #         afterstate_values = []\n",
    "    #         for _ in range(10):\n",
    "    #             env_copy = copy.deepcopy(env)\n",
    "    #             next_state, next_score, next_done, _ = env_copy.step(a)\n",
    "    #             # next_state, _, _ = move_board(env.board, a, 0)\n",
    "    #             afterstate_values.append(approximator.value(next_state))\n",
    "    #         afterstate_values_mean.append(np.mean(afterstate_values))\n",
    "    #     idx = np.argmax(afterstate_values_mean)\n",
    "    #     action = legal_moves[idx]\n",
    "    #     next_state, new_score, done, _ = env.step(action)\n",
    "\n",
    "    print(\"Game over, final score:\", env.score)\n",
    "    return env.score\n",
    "\n",
    "scores = []\n",
    "for i in range(int(30)):\n",
    "    scores.append(test_play_2048())\n",
    "print(f\"mean: {np.mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
